{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/ibm-nasa-geospatial\n",
    "\n",
    "https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accepts remote sensing data in a video format (B, C, T, H, W). he model can also handle static imagery which can be fed into the model with T=1. \n",
    "\n",
    "\n",
    "The model was pre-trained with NASA's HLS V2 L30 product (30m granularity) from the contiguous United States. The bands that were used are the following:\n",
    "\n",
    "- Blue\n",
    "- Green\n",
    "- Red\n",
    "- Narrow NIR\n",
    "- SWIR 1\n",
    "- SWIR 2\n",
    "\n",
    "Image size is 224x224. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import yaml\n",
    "from prithvi.Prithvi import MaskedAutoencoderViT\n",
    "\n",
    "NO_DATA = -9999\n",
    "NO_DATA_FLOAT = 0.0001\n",
    "PERCENTILES = (0.1, 99.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "weights_path = \"./prithvi/Prithvi_100M.pt\"\n",
    "checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
    "\n",
    "# read model config\n",
    "model_cfg_path = \"./prithvi/Prithvi_100M_config.yaml\"\n",
    "with open(model_cfg_path) as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "\n",
    "model_args, train_args = model_config[\"model_args\"], model_config[\"train_params\"]\n",
    "\n",
    "# let us use only 1 frame for now (the model was trained on 3 frames)\n",
    "model_args[\"num_frames\"] = 1\n",
    "\n",
    "# instantiate model\n",
    "model = MaskedAutoencoderViT(**model_args)\n",
    "model.eval()\n",
    "\n",
    "# load weights into model\n",
    "# strict=false since we are loading with only 1 frame, but the warning is expected\n",
    "del checkpoint['pos_embed']\n",
    "del checkpoint['decoder_pos_embed']\n",
    "_ = model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0978, grad_fn=<DivBackward0>),\n",
       " tensor([[[0.3786, 0.3884, 0.4116,  ..., 0.4292, 0.5119, 0.5209],\n",
       "          [0.4134, 0.4202, 0.4398,  ..., 0.4632, 0.4626, 0.4618],\n",
       "          [0.4605, 0.4989, 0.5138,  ..., 0.3287, 0.3848, 0.3813],\n",
       "          ...,\n",
       "          [0.3458, 0.3760, 0.3573,  ..., 0.4014, 0.7071, 0.8298],\n",
       "          [0.4271, 0.4286, 0.4310,  ..., 0.5444, 0.7183, 0.7511],\n",
       "          [0.4569, 0.4543, 0.4532,  ..., 0.3873, 0.5268, 0.5622]]],\n",
       "        grad_fn=<SliceBackward0>),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "          1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "          1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "          1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "          0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "          0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "          0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(torch.rand(1, 6, 1, 224, 224))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns a tuple with:\n",
    "\n",
    "- loss\n",
    "- reconstructed image\n",
    "- mask used\n",
    "\n",
    "Images are normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([775.2290211032589,\n",
       "  1080.992780391705,\n",
       "  1228.5855250417867,\n",
       "  2497.2022620507532,\n",
       "  2204.2139147975554,\n",
       "  1610.8324823273745],\n",
       " [1281.526139861424,\n",
       "  1270.0297974547493,\n",
       "  1399.4802505642526,\n",
       "  1368.3446143747644,\n",
       "  1291.6764008585435,\n",
       "  1154.505683480695])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# statistics used to normalize images before passing to the model\n",
    "means = train_args[\"data_mean\"]\n",
    "stds = train_args[\"data_std\"]\n",
    "\n",
    "means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finetune, you can now write a PyTorch loop as usual to train on your dataset. Simply extract the backbone from the model with some surgery and run only the model features forward, with no masking!\n",
    "\n",
    "In general some reccomendations are:\n",
    "\n",
    "- At least in the beggining, experiment with freezing the backbone. This will give you much faster iteration through experiments.\n",
    "- Err on the side of a smaller learning rate\n",
    "- With an unfrozen encoder, regularization is your friend! (Weight decay, dropout, batchnorm...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, _, _ = model.forward_encoder(torch.rand(1, 6, 1, 224, 224), mask_ratio=0)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the standard output of a ViT.\n",
    "\n",
    "- Dim 1: Batch size\n",
    "- Dim 2: [cls_token] + tokens representing flattened image\n",
    "- Dim 3: embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder features have shape torch.Size([1, 197, 768])\n",
      "Encoder features have new shape torch.Size([1, 768, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Encoder features have shape {features.shape}\")\n",
    "\n",
    "# drop cls token\n",
    "reshaped_features = features[:, 1:, :]\n",
    "\n",
    "# reshape\n",
    "feature_img_side_length = int(np.sqrt(reshaped_features.shape[1]))\n",
    "reshaped_features = reshaped_features.view(-1, feature_img_side_length, feature_img_side_length, model_args[\"embed_dim\"])\n",
    "# channels first\n",
    "reshaped_features = reshaped_features.permute(0, 3, 1, 2)\n",
    "print(f\"Encoder features have new shape {reshaped_features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
