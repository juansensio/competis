{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dm import DFTemporalDataModule\n",
    "\n",
    "trans = {\n",
    "\t'HorizontalFlip': {'p': 0.5},\n",
    "\t'VerticalFlip': {'p': 0.5},\n",
    "\t'RandomRotate90': {'p': 0.5},\n",
    "\t'Transpose': {'p': 0.5},\n",
    "\t# 'RandomResizedCrop': {'height': 256, 'width': 256, 'scale': (0.8, 1.0), 'ratio': (0.75, 1.3333333333333333), 'p': 0.5},\n",
    "\t# 'ShiftScaleRotate': {'shift_limit': 0.2, 'scale_limit': 0.2, 'rotate_limit': 30, 'p': 0.2},\n",
    "}\n",
    "\n",
    "dm = DFTemporalDataModule(batch_size=1, use_clouds=True, s2_bands=(2,1,0), train_trans=trans)\n",
    "# dm = DFTemporalDataModule(batch_size=1, train_trans=trans)\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from src.module2 import Transformer\n",
    "import torch\n",
    "\n",
    "dm = DFTemporalDataModule(batch_size=1)\n",
    "\n",
    "hparams = {\n",
    "\t'encoder': 'resnet18',\n",
    "\t'pretrained': 'imagenet',\n",
    "\t'seq_len': 12,\n",
    "\t'in_channels_s1': 2,\n",
    "\t'in_channels_s2': 3,\n",
    "    'embed_dim': 256,\n",
    "    'n_heads': 4,\n",
    "    'num_blocks': 2,\n",
    "\t'optimizer': 'Adam',\n",
    "\t'optimizer_params': {\n",
    "\t\t'lr': 1e-3\n",
    "\t},\n",
    "\t# 'scheduler': {\n",
    "\t# \t'OneCycleLR': {\n",
    "\t# \t\t'max_lr': 1e-2,\n",
    "\t# \t\t'pct_start': 0.1,\n",
    "\t# \t\t'total_steps': 1000,\n",
    "\t# \t\t# 'verbose': True\n",
    "\t# \t}\n",
    "\t# }\n",
    "}\n",
    "\n",
    "module = Transformer(hparams)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "\tgpus=1,\n",
    "\tprecision=16,\n",
    "\toverfit_batches=1,\n",
    "\tmax_epochs=300,\n",
    "\tlogger=None,\n",
    "\tenable_checkpointing=False,\n",
    ")\n",
    "\n",
    "trainer.fit(module, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a raster of the size of the output space\n",
    "# tile with positional encoding (sin, cos) \n",
    "# encode with MLP in a set of query vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 256, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# esto es lo que quiero a la salida (ser치 par치mtero y tendr치 positional encoding)\n",
    "\n",
    "a = torch.randn(16, 1, 256, 256)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# https://github.com/jankrepl/mildlyoverfitted/blob/master/github_adventures/vision_transformer/custom.py\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # (B, E, P, P)\n",
    "        x = x.flatten(2)  # (B, E, N)\n",
    "        x = x.transpose(1, 2)  # (B, N, E)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PatchEmbedding(256, 8, 1, 256)\n",
    "\n",
    "b = pe(a) # estas son mis queries\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cada imagen y multi-scale feature emite su key-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con unet\n",
    "\n",
    "mf = [\n",
    "    torch.randn(16, 64, 128, 128),\n",
    "    torch.randn(16, 64, 64, 64),\n",
    "    torch.randn(16, 128, 32, 32),\n",
    "    torch.randn(16, 256, 16, 16),\n",
    "    torch.randn(16, 512, 8, 8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 256])\n",
      "torch.Size([16, 64, 256])\n",
      "torch.Size([16, 16, 256])\n",
      "torch.Size([16, 4, 256])\n",
      "torch.Size([16, 1, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 341, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mes = [\n",
    "    PatchEmbedding(128, 8, 64, 256),\n",
    "    PatchEmbedding(64, 8, 64, 256),\n",
    "    PatchEmbedding(32, 8, 128, 256),\n",
    "    PatchEmbedding(16, 8, 256, 256),\n",
    "    PatchEmbedding(8, 8, 512, 256)\n",
    "]\n",
    "\n",
    "cs = [me(m) for me, m in zip(mes, mf)]\n",
    "for c in cs:\n",
    "    print(c.shape)\n",
    "\n",
    "c = torch.cat(cs, dim=1)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8184, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repetir para serie temporal \n",
    "# a침adir los correspondientes positional, sensor, etc encodings\n",
    "\n",
    "c2 = torch.cat([c]*12*2, dim=1)\n",
    "c2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 256])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.attention import Block\n",
    "\n",
    "block = Block(256, 256)\n",
    "d = block(c, b)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 16, 16])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve image\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "e = rearrange(d, 'b n (h w) -> b n h w', h=16, w=16)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 256, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unproj = nn.Sequential(\n",
    "    nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "    # nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "    nn.Conv2d(64, 1, kernel_size=1, stride=1)\n",
    ")\n",
    "\n",
    "f = unproj(e)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con bifpn\n",
    "\n",
    "mf = [\n",
    "    torch.randn(16, 64, 128, 128),\n",
    "    torch.randn(16, 64, 64, 64),\n",
    "    torch.randn(16, 64, 32, 32),\n",
    "    torch.randn(16, 64, 16, 16),\n",
    "    torch.randn(16, 64, 8, 8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 64])\n",
      "torch.Size([16, 64, 64])\n",
      "torch.Size([16, 16, 64])\n",
      "torch.Size([16, 4, 64])\n",
      "torch.Size([16, 1, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 341, 64])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mes = [\n",
    "    PatchEmbedding(128, 8, 64, 64),\n",
    "    PatchEmbedding(64, 8, 64, 64),\n",
    "    PatchEmbedding(32, 8, 64, 64),\n",
    "    PatchEmbedding(16, 8, 64, 64),\n",
    "    PatchEmbedding(8, 8, 64, 64)\n",
    "]\n",
    "\n",
    "cs = [me(m) for me, m in zip(mes, mf)]\n",
    "for c in cs:\n",
    "    print(c.shape)\n",
    "\n",
    "c = torch.cat(cs, dim=1)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con bifpn, si tengo dos sensores, cada sensor tiene 12 im치genes, ...\n",
    "\n",
    "mf = [\n",
    "    torch.randn(16, 2, 12, 64, 128, 128),\n",
    "    torch.randn(16, 2, 12, 64, 64, 64),\n",
    "    torch.randn(16, 2, 12, 64, 32, 32),\n",
    "    torch.randn(16, 2, 12, 64, 16, 16),\n",
    "    torch.randn(16, 2, 12, 64, 8, 8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiscaleFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b s l c h w -> b (s l c) h w')\n",
    "        x = self.proj(x)  # (B, E, P, P)\n",
    "        x = x.flatten(2)  # (B, E, N)\n",
    "        x = x.transpose(1, 2)  # (B, N, E)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256, 64])\n",
      "torch.Size([16, 64, 64])\n",
      "torch.Size([16, 16, 64])\n",
      "torch.Size([16, 4, 64])\n",
      "torch.Size([16, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "mes = [\n",
    "    MultiscaleFeatureEmbedding(128, 8, 64*2*12, 64),\n",
    "    MultiscaleFeatureEmbedding(64, 8, 64*2*12, 64),\n",
    "    MultiscaleFeatureEmbedding(32, 8, 64*2*12, 64),\n",
    "    MultiscaleFeatureEmbedding(16, 8, 64*2*12, 64),\n",
    "    MultiscaleFeatureEmbedding(8, 8, 64*2*12, 64)\n",
    "]\n",
    "\n",
    "cs = [me(m) for me, m in zip(mes, mf)]\n",
    "for c in cs:\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74dbfc52f168b3071122cf9c0781887d6121c12f9c1b29bca56ce221bccb2a07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
